{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CvgnCOPuTCc",
        "outputId": "aa4c39b1-e685-4eab-e584-c6fd0569b000"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai-whisper\n",
            "  Downloading openai-whisper-20230918.tar.gz (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.3/794.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (3.27.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (3.12.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (16.0.6)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230918-py3-none-any.whl size=798399 sha256=0b0cb5bfe55f9627e92f61eb6b0f9e8e27888bada944fe4dfd2ac5a14d4c8fe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/37/b1/9aea93201fe91e3561719120da92cc23e77b7ef6f3d0d9491a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: pytube, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20230918 pytube-15.0.0 tiktoken-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize a YouTube object\n",
        "import os\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
        "# YouTube video URL\n",
        "youtube_url = \"https://www.youtube.com/watch?v=uweW8QlXMZI&t=532s\"\n",
        "\n",
        "# Output file name\n",
        "output_file = \"output.web3\"\n",
        "\n",
        "# Create a temporary directory to store the video and audio files\n",
        "temp_dir = \"temp\"\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "yt = YouTube(youtube_url)\n",
        "title=yt.title\n",
        "name=title[:-3]+\".mp4\"\n",
        "file_location = \"/content/\"+temp_dir+\"/\"+name\n",
        "stream = yt.streams.filter(only_audio=True, file_extension=\"mp4\").first()\n",
        "stream.download(output_path=temp_dir)\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(file_location)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHBBD0XxuQFR",
        "outputId": "c772dd4a-0dfd-4d87-b762-b172a27e385f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello world and welcome to this edition of Tech on Fire with Blaze. I'm Blaze Stewart, architect at Winelike and today we're going to be taking a look at storage cues and how this simple solution can create some pretty powerful applications on Azure. Hi guys, today I want to talk about storage cues and this is one of the most overlooked features of Azure because it is oftentimes looked down on by many developers as being too primitive or too basic for their needs. But in many cases this option might be the one that you use because A, it's very inexpensive to use it and B, the kind of throughput that many applications need don't require high throughput for messaging and this option gives you a lot of variability in terms of cost and performance. So let's go ahead and look at this and why you might want to consider it for an option whenever you're building applications. So storage cues work a lot like cues on a service bus. Do you have a publisher, the cue and you have subscriber to that cue. In this case what you end up with is the publisher putting a message on the cue and then you have the subscriber. In this case asking the cue for new messages while in a service bus context the messages would just be delivered to the subscriber. And if there are messages then those get returned back to the subscriber. If there are new messages that the subscriber simply asked for new messages, this is hey, do you have any messages and the cue says no, it doesn't get any. And then the publisher might publish a new message the next time the subscriber comes in, it's going to ask for those messages and it's going to be delivered back to the subscriber. So you can see here that we have a polling style architecture. Now for many applications this is probably going to be sufficient and the advantages of doing this is because you don't have a lot of persistent connections going on with the storage cue. You basically got to figure out what that sweet spot is for how often you want to pull the cue for new messages. So that could be once a second, it could be once every five seconds, you could create some kind of back off loop. You know, if I don't get a new message then I'll wait two seconds and if I don't get a new message I'll wait four seconds. Whatever that might be, I prefer the back off method just because it prevents me from accumulating a lot of requests against the APIs that the storage cues implement. And so you don't end up running up your cost. So if your application is pretty much constantly receiving messages, then maybe you don't need something like that. You could just pretty much do it on a pretty regular interval. Maybe two seconds with one second, half a second depending on what your needs are for your application. But at the end of the day you just kind of figure out what that's going to be for your application and then implement it accordingly. And if you can live with a polling style architecture in your applications for delivering messages, then what you end up doing is saving a lot of costs on a cueing style architecture without having to pay for something like a service bus or some other messaging platform on Azure. So the biggest advantage of doing this is costs obviously. Now the tradeoff is throughput because storage cues don't have the SLAs that a service bus has or other some other messaging platforms. So you're not going to be able to scale these up to millions of messages per second. However, they will handle quite a few messages and if that's fine, then it's probably going to be more than sufficient for your application. So a lot of times when I'm writing code, I write it in such a way that I will probably default to storage cues. And if I start to meet a point where the scale of a storage queue will no longer work with my application, then I will just basically swap out the code that is implementing a storage queue for something like a service bus and just go with that. In any case, it's pretty easy to do. I'm going to show you my code and you're going to see how similar it is to what you implement in a service bus. It's pretty much the same kind of code, but you just implement polling instead. So cues in Azure are really easy to set up. There's really not much to it. So you have a storage account and it's one of the four kinds of services that are generally associated with storage accounts. You go over to queue, you add a queue and you give it a name. I'm going to call it my queue to if I wanted to or you can use the APIs to create these either way, you can easily create these and take them down without much fuss. In each one of these, you have an access policy that you can set and that will basically give you a connection stream if you so choose. Or in the case that I'm going to be doing, I'm just basically going to use the one that comes with the primary key or the secondary key for accessing these since it's already there. So I can hit show keys and grab the connection string. So the code for these again is very straightforward. So I have here a subscriber and a publisher. So the subscriber in this case and the publisher look a lot like what we'd expect from a service bus. This one, I'm basically just setting up a queue client. This is part of the SDK available to Node.js and you would have similar SDKs for Java.net and another popular library such as that. They can also be called by way of just standard APIs that are available through REST and once you have this, I'm just creating a function here that just creates a loop that publishes a message every one second. So if I'm going to run this, it will run it using the connection string and it's going to be doing it on Q1. So if I start this up, we can then just start Node and we can do index.js. And that's just going to start publishing messages to that queue. So if I come back over here to the portal, we can see these messages actually in the portal, which it's kind of a nice feature. If I go over here into my queue and click on Q1 here, I can see the messages right here inside the queue and you can clear it. You can actually add a message here if you so choose. So this is very straightforward way to see what's going on when you're doing dev work is just use the Azure portal for that. And if I refresh this, we should see more messages coming in as that particular publisher runs in the background because I don't have a subscriber yet, they're just going to continue to accumulate. So let's go ahead and start up the subscriber here. This subscriber code is pretty much like what we'd expect. In this case, though, I'm using a polling style architecture to pull the API for new messages. So I'm using the same queue, same connection string, the same client library. And this one is just pulling the API every so many seconds. So I have basically just a back off loop here. So I'm basically saying wait, however many seconds. So the defaults to one, if it comes and finds new messages up to 32, basically just resets the wait to one. Otherwise, it just doubles it. So one, two, four, eight, 16 to 32 up to a maximum, which I set up here to 32. And then basically it's going to pull every 32 seconds. And if it finds new messages, it goes back to one. So the back off loop will then reset. So that's a very common way to approach polling the API so that you're not incurring a lot of costs by just pulling it every second or half second, depending on whatever your need they be. So let's go ahead and start this guy up. And we should just get a bunch of messages since that publisher has been running in the background for a while already. So if I do know that index.js, we're going to see a bunch of messages come off. I'm going to wait another few seconds. We'll messages come off. More is coming, more coming off. And we're still getting messages. Now there we go. Now we're back up to where it's caught up with the actual publisher. And so we're seeing messages that are coming off the queue while they're being published by this, this message right here. So the publisher is 133, 133. So you know, there's a little bit of lag there, but that's mostly due to the polling nature of this application. If I was to pull more often, I would probably see those come off that queue a lot quicker. And if I come back over here to the actual page here and I refresh this, probably not going to see it. We might see one here. If I refresh this enough, but otherwise nothing staying on the queue. If the publisher and subscriber are both there, one publishing, one subscribing and removing messages as they come off the queue. So very straightforward there. So if I remove the publisher and kill that. So this is now going to start going off into a back off loop. So basically it's just doubling the amount of wait time that it has between each poll. So I'm not going to be nailing the ABI a whole bunch. And that way, I don't incur a lot of costs. It over a month period, if I'm only hitting it, say once or twice a minute, we're talking about one or two cents per month just to run that in that kind of context. If you do it about every one second, you're going to be occurring about 15 cents over the month, just to pull it every one second. So depending on whatever your needs are, you kind of figure out what that is going to look like. So if I start this back up, we'll have to wait 32 seconds before those messages start to appear over here inside of our subscriber. But whenever that happens, it should run all the messages that have been acute up to that point. And then we will get a list of messages that will reset the back off loop to one second. However, along that might be. And there they go. Now it's just going to be running those messages as they come off the Q there. So again, very straightforward, but that's how Qs work. It's really not much to it, but they're also very powerful and also very inexpensive to run on Azure. So with this, you can create very inexpensive messaging infrastructure. And if it serves your application, then that's what I would encourage you to use as a default. And if it doesn't scale well enough to your needs, then you can go to something like service bus. If you like this content, please consider visiting us online at www.windtaleag.com. And there you can find about services that wind elect offers, including training and consulting services. Also, please consider subscribing to this channel by clicking on the subscribe button and clicking the bell icon to get notifications when new content becomes available. And also comment down below. You can also follow me on Twitter at the one view and also follow wind elect, album Twitter at wind elect now or at win elect. We are constantly posting things about Azure related technologies and things related to software development. You can also reach us by email at www.windtaleag.com. Until next time, thank you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OT0GiTJGyOSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "toX1M7iIx93E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}